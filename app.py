import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

# === 1. ç½‘é¡µåŸºç¡€è®¾ç½® ===
st.set_page_config(
    page_title="æ ¡æ‹›æƒ…æŠ¥å±€", 
    page_icon="ğŸ“",
    layout="wide"
)

st.title("ğŸ“ 2026 æ ¡æ‹›/å®ä¹ æƒ…æŠ¥å±€")
st.markdown("""
è¿™é‡Œæ˜¯ä½ çš„ä¸ªäººæƒ…æŠ¥ä¸­å¿ƒã€‚ç‚¹å‡»å·¦ä¾§çš„ **â€œå¼€å§‹æŠ“å–â€** æŒ‰é’®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä» *GiveMeOC* è·å–æœ€æ–°å²—ä½ã€‚
""")

# === 2. æ ¸å¿ƒçˆ¬è™«åŠŸèƒ½ (è¿™å°±æ˜¯åˆšæ‰ä½ å†™çš„é‚£ä¸ªçˆ¬è™«) ===
def run_spider(max_pages):
    base_url = "http://www.givemeoc.com/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    all_data = []
    current_url = base_url
    
    # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for page in range(1, max_pages + 1):
        status_text.text(f"æ­£åœ¨æ‰«æç¬¬ {page}/{max_pages} é¡µ...")
        progress_bar.progress(page / max_pages)
        
        try:
            response = requests.get(current_url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                rows = soup.find_all('tr')
                
                for row in rows[1:]:
                    try:
                        cols = row.find_all('td')
                        if len(cols) >= 11:
                            # --- é“¾æ¥æ¸…æ´—é€»è¾‘ ---
                            apply_link = "æ— é“¾æ¥"
                            apply_col = cols[10].find('a')
                            if apply_col and 'href' in apply_col.attrs:
                                raw_href = apply_col['href'].strip()
                                # ä¿®å¤è„æ•°æ®
                                if "é“¾æ¥æŠ•é€’" in raw_href:
                                    raw_href = raw_href.replace("é“¾æ¥æŠ•é€’", "").replace(":", "").replace("ï¼š", "").strip()
                                try:
                                    apply_link = urljoin(base_url, raw_href)
                                except:
                                    apply_link = "é“¾æ¥é”™è¯¯"
                            
                            info = {
                                "å…¬å¸": cols[0].text.strip(),
                                "æŠ•é€’ç›´è¾¾": apply_link, # æ”¾åœ¨å‰é¢æ–¹ä¾¿ç‚¹
                                "å²—ä½": cols[6].text.strip(),
                                "åœ°ç‚¹": cols[5].text.strip(),
                                "æˆªæ­¢æ—¥æœŸ": cols[9].text.strip(),
                                "ç±»å‹": cols[3].text.strip(),
                                "è¡Œä¸š": cols[2].text.strip(),
                                "å‘å¸ƒæ—¶é—´": cols[8].text.strip()
                            }
                            all_data.append(info)
                    except:
                        continue
                
                # å¯»æ‰¾ä¸‹ä¸€é¡µ
                next_page_found = False
                all_links = soup.find_all('a')
                for link in all_links:
                    if "ä¸‹ä¸€é¡µ" in link.text or "Â»" in link.text:
                        next_url = link.get('href')
                        if next_url:
                            current_url = urljoin(base_url, next_url)
                            next_page_found = True
                            break
                
                if not next_page_found:
                    st.warning("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
            else:
                st.error(f"ç¬¬ {page} é¡µè®¿é—®å¤±è´¥")
                
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
            break
            
    progress_bar.empty() # æŠ“å®Œåéšè—è¿›åº¦æ¡
    status_text.text("âœ… æŠ“å–å®Œæˆï¼")
    return pd.DataFrame(all_data)

# === 3. ä¾§è¾¹æ æ§åˆ¶åŒº ===
with st.sidebar:
    st.header("ğŸ® æ§åˆ¶å°")
    pages_to_crawl = st.slider("æŠ“å–å¤šå°‘é¡µï¼Ÿ", 1, 50, 5) # é»˜è®¤æŠ“5é¡µ
    
    if st.button("ğŸš€ å¼€å§‹æŠ“å–æ•°æ®", type="primary"):
        with st.spinner('æ­£åœ¨ç–¯ç‹‚çˆ¬å–ä¸­ï¼Œè¯·ç¨ç­‰...'):
            df = run_spider(pages_to_crawl)
            # æŠŠæ•°æ®å­˜åˆ° session_state é‡Œï¼Œè¿™æ ·åˆ·æ–°ç½‘é¡µæ•°æ®ä¸ä¼šä¸¢
            st.session_state['data'] = df
            st.success(f"æˆåŠŸè·å– {len(df)} æ¡å²—ä½ä¿¡æ¯ï¼")

# === 4. æ•°æ®å±•ç¤ºåŒº ===
if 'data' in st.session_state:
    df = st.session_state['data']
    
    # ç®€å•çš„ç­›é€‰å™¨
    search_term = st.text_input("ğŸ” æœç´¢å…¬å¸æˆ–å²—ä½ (ä¾‹å¦‚: è…¾è®¯ / Java)", "")
    if search_term:
        df = df[df.apply(lambda row: row.astype(str).str.contains(search_term, case=False).any(), axis=1)]
    
    # å±•ç¤ºæ¼‚äº®çš„è¡¨æ ¼ï¼Œé“¾æ¥å¯ç‚¹å‡»
    st.dataframe(
        df,
        column_config={
            "æŠ•é€’ç›´è¾¾": st.column_config.LinkColumn("ç‚¹å‡»æŠ•é€’"),
        },
        use_container_width=True,
        height=600
    )
    
    # ä¸‹è½½æŒ‰é’®
    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        "ğŸ’¾ ä¸‹è½½ä¸º Excel/CSV",
        csv,
        "æ ¡æ‹›æ•°æ®.csv",
        "text/csv",
        key='download-csv'
    )
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ è®¾ç½®é¡µæ•°ï¼Œå¹¶ç‚¹å‡»æŒ‰é’®å¼€å§‹æŠ“å–ã€‚")
